{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples shape: (908, 4000, 3), labels shape: (908,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ds = np.load('modelnet10_val.npz')\n",
    "samples = ds['samples']\n",
    "labels = ds['labels']\n",
    "print(\"samples shape: %s, labels shape: %s\" %(samples.shape, labels.shape))\n",
    "\n",
    "dim =32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pointcloud2volume(pc, dim=32):\n",
    "    vol = np.zeros((dim,dim,dim))\n",
    "    tempPC = np.copy(pc)\n",
    "    tempPC *= (dim-1) \n",
    "    tempPC = tempPC.astype(int)\n",
    "    vol[tempPC[:,0],tempPC[:,1],tempPC[:,2]] = 1.0\n",
    "    return vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vol2depthmap(v, bg_val=40.):\n",
    "    temp0=np.zeros((dim,dim,7))\n",
    "    temp1=np.ones((dim,dim,1))\n",
    "    temp=np.concatenate((temp0, temp1),axis=2)\n",
    "    temp=np.concatenate((v, temp),axis=2)\n",
    "    depthmap = temp.argmax(2)\n",
    "  \n",
    "    return depthmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x34daff9a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASoAAAD8CAYAAADAKumpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOVGZWfKcqMwseU5UZpY8JyozS97/ASuW2nPC9kOxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x34e1ee3b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = pointcloud2volume(samples[4])\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(vol2depthmap(v), cmap='jet')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printDepthMap (im):\n",
    "    %matplotlib inline\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.imshow(im, cmap='jet')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CreateDepthMapDataset (samples):\n",
    "    DepthMapDataset= np.zeros((len(samples),dim,dim)) \n",
    "    for i in range(len(samples)):\n",
    "        vol=pointcloud2volume(samples[i])\n",
    "        temp=np.array([vol2depthmap(vol)])\n",
    "        DepthMapDataset[i] = temp\n",
    "    return DepthMapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3991, 32, 32, 1)\n",
      "3991 train samples\n",
      "908 test samples\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib as matplotlib\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = dim,dim\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "'''load val'''\n",
    "samplesData = CreateDepthMapDataset (samples)\n",
    "\n",
    "'''load train'''\n",
    "modelnet10_train = np.load('modelnet10_train.npz')\n",
    "samplesTrain = modelnet10_train['samples']\n",
    "labelsTrain = modelnet10_train['labels']\n",
    "samplesDataTrain = CreateDepthMapDataset (samplesTrain)\n",
    "\n",
    "    \n",
    "(x_train, y_train)=(samplesDataTrain, labelsTrain)\n",
    "(x_test, y_test) = (samplesData, labels)\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3991 samples, validate on 908 samples\n",
      "Epoch 1/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 2.0246 - acc: 0.2829 - val_loss: 1.8279 - val_acc: 0.3359\n",
      "Epoch 2/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 1.2430 - acc: 0.5998 - val_loss: 1.5032 - val_acc: 0.4703\n",
      "Epoch 3/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.9346 - acc: 0.7078 - val_loss: 1.0538 - val_acc: 0.6355\n",
      "Epoch 4/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.7805 - acc: 0.7492 - val_loss: 1.0294 - val_acc: 0.6817\n",
      "Epoch 5/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.7027 - acc: 0.7682 - val_loss: 0.8021 - val_acc: 0.7368\n",
      "Epoch 6/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.6385 - acc: 0.7940 - val_loss: 0.7570 - val_acc: 0.7357\n",
      "Epoch 7/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.6074 - acc: 0.7995 - val_loss: 0.7614 - val_acc: 0.7379\n",
      "Epoch 8/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.5676 - acc: 0.8111 - val_loss: 0.8328 - val_acc: 0.7291\n",
      "Epoch 9/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.5395 - acc: 0.8241 - val_loss: 0.8462 - val_acc: 0.7048\n",
      "Epoch 10/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.5186 - acc: 0.8294 - val_loss: 0.8155 - val_acc: 0.7291\n",
      "Epoch 11/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.4961 - acc: 0.8369 - val_loss: 0.7259 - val_acc: 0.7445\n",
      "Epoch 12/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.4835 - acc: 0.8391 - val_loss: 0.7674 - val_acc: 0.7456\n",
      "Epoch 13/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.4626 - acc: 0.8482 - val_loss: 0.6897 - val_acc: 0.7621\n",
      "Epoch 14/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.4510 - acc: 0.8522 - val_loss: 0.6860 - val_acc: 0.7610\n",
      "Epoch 15/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.4443 - acc: 0.8534 - val_loss: 0.6727 - val_acc: 0.7687\n",
      "Epoch 16/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.4229 - acc: 0.8599 - val_loss: 0.6592 - val_acc: 0.7720\n",
      "Epoch 17/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.4178 - acc: 0.8592 - val_loss: 0.6172 - val_acc: 0.7919\n",
      "Epoch 18/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.3997 - acc: 0.8717 - val_loss: 0.6714 - val_acc: 0.7599\n",
      "Epoch 19/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.4010 - acc: 0.8664 - val_loss: 0.6554 - val_acc: 0.7632\n",
      "Epoch 20/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.3924 - acc: 0.8637 - val_loss: 0.6081 - val_acc: 0.7930\n",
      "Epoch 21/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.3801 - acc: 0.8680 - val_loss: 0.6042 - val_acc: 0.7930\n",
      "Epoch 22/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.3673 - acc: 0.8717 - val_loss: 0.5842 - val_acc: 0.8073\n",
      "Epoch 23/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.3710 - acc: 0.8775 - val_loss: 0.6692 - val_acc: 0.7764\n",
      "Epoch 24/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.3656 - acc: 0.8762 - val_loss: 0.6438 - val_acc: 0.7830\n",
      "Epoch 25/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.3458 - acc: 0.8805 - val_loss: 0.6180 - val_acc: 0.7907\n",
      "Epoch 26/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.3540 - acc: 0.8787 - val_loss: 0.6309 - val_acc: 0.7599\n",
      "Epoch 27/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.3442 - acc: 0.8797 - val_loss: 0.6245 - val_acc: 0.7819\n",
      "Epoch 28/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.3450 - acc: 0.8785 - val_loss: 0.6363 - val_acc: 0.7797\n",
      "Epoch 29/100\n",
      "3991/3991 [==============================] - 25s 6ms/step - loss: 0.3340 - acc: 0.8800 - val_loss: 0.6471 - val_acc: 0.7742\n",
      "Epoch 30/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.3307 - acc: 0.8860 - val_loss: 0.5807 - val_acc: 0.8007\n",
      "Epoch 31/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.3277 - acc: 0.8877 - val_loss: 0.6420 - val_acc: 0.7808\n",
      "Epoch 32/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.3173 - acc: 0.8852 - val_loss: 0.5852 - val_acc: 0.7963\n",
      "Epoch 33/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.3086 - acc: 0.8970 - val_loss: 0.6124 - val_acc: 0.8029\n",
      "Epoch 34/100\n",
      "3991/3991 [==============================] - 22s 6ms/step - loss: 0.3033 - acc: 0.8960 - val_loss: 0.5623 - val_acc: 0.8150\n",
      "Epoch 35/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.3121 - acc: 0.8895 - val_loss: 0.6003 - val_acc: 0.7996\n",
      "Epoch 36/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2946 - acc: 0.9015 - val_loss: 0.6223 - val_acc: 0.7753\n",
      "Epoch 37/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2942 - acc: 0.9003 - val_loss: 0.5942 - val_acc: 0.8062\n",
      "Epoch 38/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.2902 - acc: 0.8995 - val_loss: 0.6067 - val_acc: 0.8040\n",
      "Epoch 39/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2782 - acc: 0.9013 - val_loss: 0.5679 - val_acc: 0.8139\n",
      "Epoch 40/100\n",
      "3991/3991 [==============================] - 26s 7ms/step - loss: 0.2729 - acc: 0.9030 - val_loss: 0.6423 - val_acc: 0.7974\n",
      "Epoch 41/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2701 - acc: 0.9023 - val_loss: 0.5869 - val_acc: 0.8084\n",
      "Epoch 42/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.2696 - acc: 0.9028 - val_loss: 0.6059 - val_acc: 0.8161\n",
      "Epoch 43/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.2726 - acc: 0.9043 - val_loss: 0.6337 - val_acc: 0.7709\n",
      "Epoch 44/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2661 - acc: 0.9058 - val_loss: 0.6050 - val_acc: 0.8040\n",
      "Epoch 45/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.2591 - acc: 0.9060 - val_loss: 0.6016 - val_acc: 0.8095\n",
      "Epoch 46/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2523 - acc: 0.9083 - val_loss: 0.6308 - val_acc: 0.7930\n",
      "Epoch 47/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2468 - acc: 0.9136 - val_loss: 0.5843 - val_acc: 0.8106\n",
      "Epoch 48/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2511 - acc: 0.9110 - val_loss: 0.5902 - val_acc: 0.7996\n",
      "Epoch 49/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2465 - acc: 0.9168 - val_loss: 0.6163 - val_acc: 0.8018\n",
      "Epoch 50/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2267 - acc: 0.9208 - val_loss: 0.5661 - val_acc: 0.8150\n",
      "Epoch 51/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.2411 - acc: 0.9161 - val_loss: 0.5883 - val_acc: 0.8106\n",
      "Epoch 52/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.2203 - acc: 0.9231 - val_loss: 0.6198 - val_acc: 0.8062\n",
      "Epoch 53/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.2324 - acc: 0.9186 - val_loss: 0.6084 - val_acc: 0.8128\n",
      "Epoch 54/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.2235 - acc: 0.9196 - val_loss: 0.6695 - val_acc: 0.7930\n",
      "Epoch 55/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2225 - acc: 0.9213 - val_loss: 0.6130 - val_acc: 0.8007\n",
      "Epoch 56/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2114 - acc: 0.9251 - val_loss: 0.6267 - val_acc: 0.8018\n",
      "Epoch 57/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2174 - acc: 0.9218 - val_loss: 0.6206 - val_acc: 0.7985\n",
      "Epoch 58/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2113 - acc: 0.9268 - val_loss: 0.6111 - val_acc: 0.8172\n",
      "Epoch 59/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2176 - acc: 0.9218 - val_loss: 0.6507 - val_acc: 0.8007\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2152 - acc: 0.9246 - val_loss: 0.6000 - val_acc: 0.8128\n",
      "Epoch 61/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.2000 - acc: 0.9328 - val_loss: 0.6123 - val_acc: 0.8040\n",
      "Epoch 62/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2040 - acc: 0.9263 - val_loss: 0.6316 - val_acc: 0.7985\n",
      "Epoch 63/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2036 - acc: 0.9283 - val_loss: 0.6342 - val_acc: 0.8139\n",
      "Epoch 64/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.2027 - acc: 0.9256 - val_loss: 0.6391 - val_acc: 0.8040\n",
      "Epoch 65/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1950 - acc: 0.9339 - val_loss: 0.6342 - val_acc: 0.8095\n",
      "Epoch 66/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1890 - acc: 0.9359 - val_loss: 0.6397 - val_acc: 0.8007\n",
      "Epoch 67/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1897 - acc: 0.9334 - val_loss: 0.5827 - val_acc: 0.8282\n",
      "Epoch 68/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1812 - acc: 0.9374 - val_loss: 0.5914 - val_acc: 0.8282\n",
      "Epoch 69/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1853 - acc: 0.9331 - val_loss: 0.6889 - val_acc: 0.7985\n",
      "Epoch 70/100\n",
      "3991/3991 [==============================] - 22s 6ms/step - loss: 0.1821 - acc: 0.9384 - val_loss: 0.6341 - val_acc: 0.8216\n",
      "Epoch 71/100\n",
      "3991/3991 [==============================] - 25s 6ms/step - loss: 0.1724 - acc: 0.9384 - val_loss: 0.6319 - val_acc: 0.8172\n",
      "Epoch 72/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1777 - acc: 0.9374 - val_loss: 0.5619 - val_acc: 0.8326\n",
      "Epoch 73/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1786 - acc: 0.9374 - val_loss: 0.5917 - val_acc: 0.8337\n",
      "Epoch 74/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1700 - acc: 0.9394 - val_loss: 0.5928 - val_acc: 0.8304\n",
      "Epoch 75/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1704 - acc: 0.9376 - val_loss: 0.6538 - val_acc: 0.8183\n",
      "Epoch 76/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1655 - acc: 0.9411 - val_loss: 0.6188 - val_acc: 0.8282\n",
      "Epoch 77/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1703 - acc: 0.9384 - val_loss: 0.7319 - val_acc: 0.7963\n",
      "Epoch 78/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1624 - acc: 0.9401 - val_loss: 0.6269 - val_acc: 0.8249\n",
      "Epoch 79/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1552 - acc: 0.9464 - val_loss: 0.6141 - val_acc: 0.8183\n",
      "Epoch 80/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1625 - acc: 0.9414 - val_loss: 0.6551 - val_acc: 0.8161\n",
      "Epoch 81/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1544 - acc: 0.9446 - val_loss: 0.6259 - val_acc: 0.8348\n",
      "Epoch 82/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1501 - acc: 0.9439 - val_loss: 0.6275 - val_acc: 0.8161\n",
      "Epoch 83/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1531 - acc: 0.9436 - val_loss: 0.6352 - val_acc: 0.8227\n",
      "Epoch 84/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1463 - acc: 0.9429 - val_loss: 0.6235 - val_acc: 0.8260\n",
      "Epoch 85/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1517 - acc: 0.9504 - val_loss: 0.6859 - val_acc: 0.8227\n",
      "Epoch 86/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1496 - acc: 0.9449 - val_loss: 0.6227 - val_acc: 0.8414\n",
      "Epoch 87/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1450 - acc: 0.9499 - val_loss: 0.6907 - val_acc: 0.8172\n",
      "Epoch 88/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1422 - acc: 0.9516 - val_loss: 0.6926 - val_acc: 0.8194\n",
      "Epoch 89/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1422 - acc: 0.9496 - val_loss: 0.6396 - val_acc: 0.8326\n",
      "Epoch 90/100\n",
      "3991/3991 [==============================] - 26s 7ms/step - loss: 0.1376 - acc: 0.9541 - val_loss: 0.6618 - val_acc: 0.8271\n",
      "Epoch 91/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1439 - acc: 0.9439 - val_loss: 0.7335 - val_acc: 0.8007\n",
      "Epoch 92/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1441 - acc: 0.9496 - val_loss: 0.6412 - val_acc: 0.8271\n",
      "Epoch 93/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1270 - acc: 0.9562 - val_loss: 0.6876 - val_acc: 0.8161\n",
      "Epoch 94/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1342 - acc: 0.9541 - val_loss: 0.6636 - val_acc: 0.8238\n",
      "Epoch 95/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1324 - acc: 0.9511 - val_loss: 0.6426 - val_acc: 0.8370\n",
      "Epoch 96/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1273 - acc: 0.9521 - val_loss: 0.6833 - val_acc: 0.8216\n",
      "Epoch 97/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1295 - acc: 0.9551 - val_loss: 0.6763 - val_acc: 0.8293\n",
      "Epoch 98/100\n",
      "3991/3991 [==============================] - 24s 6ms/step - loss: 0.1249 - acc: 0.9529 - val_loss: 0.6859 - val_acc: 0.8249\n",
      "Epoch 99/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1262 - acc: 0.9536 - val_loss: 0.6789 - val_acc: 0.8183\n",
      "Epoch 100/100\n",
      "3991/3991 [==============================] - 23s 6ms/step - loss: 0.1213 - acc: 0.9584 - val_loss: 0.6602 - val_acc: 0.8348\n",
      "Test loss: 0.660182959248\n",
      "Test accuracy: 0.834801762377\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib as matplotlib\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "activation='relu',\n",
    "input_shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
